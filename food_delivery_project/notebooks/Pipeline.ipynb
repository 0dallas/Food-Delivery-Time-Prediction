{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "625e1be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff634a97",
   "metadata": {},
   "source": [
    "## data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7903b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath:str)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the dataset from a CSV file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac0dfc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df:pd.DataFrame, random_state:int = 42):\n",
    "    \"\"\"\n",
    "    Performs data preprocessing:\n",
    "    - Handling missing values.\n",
    "    - Coding categorical variables.\n",
    "    - Scaling numerical variables.\n",
    "    - Dividing into training and test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Identify numeric and categorical columns\n",
    "    numerical_features = ['Distance_km', 'Preparation_Time_min', 'Courier_Experience_yrs']\n",
    "    categorical_features = ['Weather', 'Traffic_Level', 'Time_of_Day', 'Vehicle_Type']\n",
    "\n",
    "    ## Create transformers for preprocessing\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer',SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    ## Create a preprocessor that applies the transformations\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num',numeric_transformer,numerical_features),\n",
    "            ('cat',categorical_transformer,categorical_features)\n",
    "        ],\n",
    "     \n",
    "    )\n",
    "\n",
    "    X = df.drop(columns=['Delivery_Time_min'])\n",
    "    y = df['Delivery_Time_min']\n",
    "\n",
    "    ## Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "    ## Fit the preprocessor\n",
    "    preprocessor.fit(X_train)\n",
    "    X_train_processed = preprocessor.transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    logging.info(\"Data successfully preprocessed\")\n",
    "\n",
    "    ## Save the preprocessor\n",
    "    joblib.dump(preprocessor, \"../models/preprocessor.pkl\")\n",
    "    logging.info(\"Preprocessor saved successfully\")\n",
    "\n",
    "    num_features = preprocessor.transformers_[0][2]\n",
    "    ohe = preprocessor.transformers_[1][1]\n",
    "    cat_features = preprocessor.transformers_[1][2]\n",
    "    ohe_feature_names = ohe.get_feature_names_out(cat_features)\n",
    "    all_feature_names = list(num_features) + list(ohe_feature_names)\n",
    "\n",
    "    ## Convert the processed data with the new columns\n",
    "    X_train_processed_df = pd.DataFrame(X_train_processed,columns=all_feature_names)\n",
    "    X_test_processed_df = pd.DataFrame(X_test_processed,columns=all_feature_names)\n",
    "\n",
    "    return X_train_processed_df, X_test_processed_df, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "159d380c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 22:26:33,415 - root - INFO - Data successfully preprocessed\n",
      "2025-06-30 22:26:33,421 - root - INFO - Preprocessor saved successfully\n"
     ]
    }
   ],
   "source": [
    "df = load_data(\"../data/Food_Delivery_Times.csv\")\n",
    "X_train, X_test, y_train, y_test = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a05a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f003114",
   "metadata": {},
   "source": [
    "## model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b5ee0c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import optuna\n",
    "import joblib\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "# current_script_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "best_model_scores = defaultdict(lambda: float(\"inf\"))\n",
    "best_model_params = {}\n",
    "\n",
    "def objective(trial,X,y):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"elasticnet\",\"random_forest\", \"svm\",\"lgbm\", \"xgb\"])\n",
    "\n",
    "    if model_name == \"elasticnet\":\n",
    "        alpha = trial.suggest_float(\"alpha\", 1e-4, 10.0, log=True)\n",
    "        l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0)\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio,max_iter=5000, random_state=42)\n",
    "\n",
    "    if model_name == \"random_forest\":\n",
    "        n_estimators = trial.suggest_int(\"rf_n_estimators\", 100, 1000)\n",
    "        max_depth = trial.suggest_int(\"rf_max_depth\", 3, 30)\n",
    "        model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "\n",
    "    elif model_name == \"svm\":\n",
    "        C = trial.suggest_float(\"svm_C\", 0.1, 100.0, log=True)\n",
    "        epsilon = trial.suggest_float(\"svm_epsilon\", 0.01, 1.0, log=True)\n",
    "        kernel = trial.suggest_categorical(\"svm_kernel\", [\"linear\", \"rbf\"])\n",
    "        model = SVR(C=C, epsilon=epsilon, kernel=kernel)\n",
    "\n",
    "    elif model_name == \"xgb\":\n",
    "        n_estimators = trial.suggest_int(\"xgb_n_estimators\", 100, 1000)\n",
    "        max_depth = trial.suggest_int(\"xgb_max_depth\", 3, 30)\n",
    "        learning_rate = trial.suggest_float(\"xgb_lr\", 0.01, 0.3)\n",
    "        model = XGBRegressor(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                             learning_rate=learning_rate, random_state=42,\n",
    "                             objective=\"reg:squarederror\", verbosity=0)\n",
    "\n",
    "    elif model_name == \"lgbm\":\n",
    "        n_estimators = trial.suggest_int(\"lgb_n_estimators\", 100, 1000)\n",
    "        max_depth = trial.suggest_int(\"lgb_max_depth\", 3, 30)\n",
    "        learning_rate = trial.suggest_float(\"lgb_lr\", 0.01, 0.3)\n",
    "        model = LGBMRegressor(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                              learning_rate=learning_rate, random_state=42)\n",
    "        \n",
    "    ## Cross-validation MAE\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mae = cross_val_score(model, X, y, cv=cv, scoring=make_scorer(mean_absolute_error)).mean()\n",
    "    \n",
    "    ## Save if it's better for that model\n",
    "    if mae < best_model_scores[model_name]:\n",
    "        best_model_scores[model_name] = mae\n",
    "        best_model_params[model_name] = trial.params\n",
    "\n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a9ae1a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_best_model(name, params):\n",
    "    if name == \"elasticnet\":\n",
    "        return ElasticNet(alpha=params['alpha'], l1_ratio=params['l1_ratio'], random_state=42)\n",
    "    elif name == \"random_forest\":\n",
    "        return RandomForestRegressor(n_estimators=params['rf_n_estimators'],\n",
    "                                     max_depth=params['rf_max_depth'], random_state=42)\n",
    "    elif name == \"svm\":\n",
    "        return SVR(C=params['svm_C'], epsilon=params['svm_epsilon'], kernel=params['svm_kernel'])\n",
    "    elif name == \"lgbm\":\n",
    "        return LGBMRegressor(n_estimators=params['lgb_n_estimators'],\n",
    "                             max_depth=params['lgb_max_depth'],\n",
    "                             learning_rate=params['lgb_lr'],\n",
    "                             random_state=42)\n",
    "    elif name == \"xgb\":\n",
    "        return XGBRegressor(n_estimators=params['xgb_n_estimators'],\n",
    "                            max_depth=params['xgb_max_depth'],\n",
    "                            learning_rate=params['xgb_lr'],\n",
    "                            random_state=42,\n",
    "                            objective=\"reg:squarederror\", verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train_processed, y_train):\n",
    "    \"\"\"\n",
    "    Uses optuna for hyperparameter tuning\n",
    "    \"\"\"\n",
    "    objective_with_data = partial(objective, X=X_train_processed, y=y_train)\n",
    "\n",
    "    logging.info(\"Hyperparameterization with OPTUNA\")\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective_with_data, n_trials=50)\n",
    "    logging.info(\"Training completed\")\n",
    "\n",
    "    print(\"Best MAE by model\")\n",
    "    for model_name, mae in best_model_scores.items():\n",
    "        print(f\"  {model_name}: {mae:.4f}\")\n",
    "\n",
    "    print(\"Details best model\")\n",
    "    best_model_name = study.best_params['model']\n",
    "    print(f\"  Model: {best_model_name}\")\n",
    "    print(f\"  MAE: {study.best_value:.4f}\")\n",
    "    print(f\"  Hyperparameters: {study.best_params}\")\n",
    "\n",
    "    final_model = build_best_model(best_model_name, study.best_params)\n",
    "    final_model.fit(X_train_processed, y_train)\n",
    "\n",
    "    # joblib.dump(final_model, os.path.join(current_script_directory,\"..\",\"models\",\"model.pkl\"))\n",
    "    joblib.dump(final_model, \"../models/model.pkl\")\n",
    "    logging.info(\"Model saved successfully\")\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fcc2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 00:23:11,598 - root - INFO - Hyperparameterization with OPTUNA\n",
      "2025-07-01 00:24:34,440 - root - INFO - Training completed\n",
      "2025-07-01 00:24:34,502 - root - INFO - Model saved successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MAE by model\n",
      "  elasticnet: 6.8095\n",
      "  xgb: 8.4059\n",
      "  svm: 6.5549\n",
      "  lgbm: 8.5493\n",
      "  random_forest: 7.9636\n",
      "Details best model\n",
      "  Modelo: svm\n",
      "  MAE: 6.5549\n",
      "  Hiperparámetros: {'model': 'svm', 'svm_C': 5.7345428219985575, 'svm_epsilon': 0.057104846911244816, 'svm_kernel': 'linear'}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "model = train_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ae0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af140cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Downloading shap-0.48.0-cp313-cp313-win_amd64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from shap) (2.3.1)\n",
      "Requirement already satisfied: scipy in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from shap) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from shap) (1.7.0)\n",
      "Requirement already satisfied: pandas in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from shap) (2.3.0)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from shap) (25.0)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting numba>=0.54 (from shap)\n",
      "  Downloading numba-0.61.2-cp313-cp313-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting cloudpickle (from shap)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from shap) (4.14.0)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.54->shap)\n",
      "  Downloading llvmlite-0.44.0-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting numpy (from shap)\n",
      "  Downloading numpy-2.2.6-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from scikit-learn->shap) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\concursos\\pg\\food-delivery-time-prediction\\main\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "Downloading shap-0.48.0-cp313-cp313-win_amd64.whl (545 kB)\n",
      "   ---------------------------------------- 0.0/545.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 545.1/545.1 kB 20.9 MB/s eta 0:00:00\n",
      "Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Downloading numba-0.61.2-cp313-cp313-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.8/2.8 MB 70.6 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.6-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 10.2/12.6 MB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 39.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 29.7 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading llvmlite-0.44.0-cp313-cp313-win_amd64.whl (30.3 MB)\n",
      "   ---------------------------------------- 0.0/30.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 6.8/30.3 MB 34.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 16.0/30.3 MB 38.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 21.8/30.3 MB 34.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 27.0/30.3 MB 31.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 30.3/30.3 MB 28.4 MB/s eta 0:00:00\n",
      "Installing collected packages: slicer, numpy, llvmlite, cloudpickle, numba, shap\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.1\n",
      "    Uninstalling numpy-2.3.1:\n",
      "      Successfully uninstalled numpy-2.3.1\n",
      "Successfully installed cloudpickle-3.1.1 llvmlite-0.44.0 numba-0.61.2 numpy-2.2.6 shap-0.48.0 slicer-0.0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\CONCURSOS\\PG\\Food-Delivery-Time-Prediction\\main\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\CONCURSOS\\PG\\Food-Delivery-Time-Prediction\\main\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
